# Nonparametric-Regression-And-Classification

Introduction

This was the final project during the Nonparametric Regression And Classification class at UW. My goal is to compare the prediction performances between the Negative Binomial GLM model and common machine learning techniques, including lasso regression, ridge regression, general additive model, partial least square regression, XGBoost, KNN for Regression, stochastic gradient boosting, bagging, elasticnet, ensemble, boosting, and neural network. All codes are written in R using the packages lme4 and Caret. The evaluation criterion for choosing the best machine learning model is RMSE (root-mean-square error).
The data set contains information on clickstream from an online store offering clothing for pregnant women. Data are from five months of 2008 and include, among others, product category, location of the photo on the page, country of origin of the IP address, and product price in US dollars. There are 132380 observations and 14 variables with no missing values in the original data set.

Conclusion

I found that most models perform evenly concerning the prediction performance. Yet, if we compare the best model in each method, XGBoost is the best, and partial least squares regression has the worst result. The potential reason for this might be that outliers have minimal impact, and it handles large-sized data sets well. This is true since most of the number of orders is small. I also found that people on Kaggle win most competitions with XGBoost.
On the other hand, stochastic gradient boosting is also a competitive candidate. These are the two methods that surpass other candidates. Since the outcome variable consists of all integers, I also calculated the result of rounding the prediction to integers. The table indicates no evident difference in the two tables regarding the prediction performance.

Generally, most of the models perform well. Still, depending on how many hyperparameters we plan to tune, the time to produce results may vary. While it took about 40 seconds to run the linear mixed model, it still could not compete with other contenders. Once having a more in-depth understanding of the data, the more appropriate cluster structure can derive a better result. On the other hand, even with the support of parallel computing, I gave up fitting the random forest model since it took forever to generate the result and tune hyperparameters. The reason may be that there are more than 13 thousand observations in the data set.

Additionally, I eventually quit fitting the support vector machine, as the kernel matrix's space complexity is O(n2). When n is in order of 105, getting SVM to work is almost impossible since it would require much memory that my laptop can not tackle. Given that the outcome variable is an integer, I have read some forums where people treat it as a categorical variable instead. This might provide us with more powerful insight into selecting the optimal model. To sum up, through taking this course, I feel that people always strive to reduce all sorts of losses by dividing data with different thresholds or using gradient descent in the nonparametric world. By combining various techniques, a new and powerful method might be born.
